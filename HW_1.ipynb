{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csw5866/hello/blob/main/HW_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcRxf1gUt3Kr"
      },
      "source": [
        "# Q1. AlexNet with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v1YhH_4t3Ku"
      },
      "source": [
        "### Implementing AlexNet\n",
        "\n",
        "> 1. **Dataset**\n",
        ">> - We use CIFAR-100 (32x32 --> 224x224) (Due to the computational constraints.)\n",
        ">> - You need to resize the input dataset\n",
        ">> - 10 Epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# CIFAR-100 stats (commonly used)\n",
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "# Transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# Datasets / Loaders\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True,\n",
        "    num_workers=1, pin_memory=True,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=128, shuffle=False,\n",
        "    num_workers=1, pin_memory=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "QtzxVmOst3Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09774000-7c66-4952-ffc4-dd673cb853b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "\n",
        "\n",
        "class Alexnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Alexnet,self).__init__()\n",
        "\n",
        "    #conv 1 (48kernels for each gpu)\n",
        "    self.cnn1= nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4)\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.lrn1=nn.LocalResponseNorm(size=5,alpha=1e-4,beta=0.75,k=2.0)\n",
        "\n",
        "    #max pool 1\n",
        "    self.maxpool1=nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    #conv 2 (gpu1->1, gpu2->2, 128 kernels for each gpu)\n",
        "    self.cnn2=nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5,stride=1, padding=2)\n",
        "    self.relu2=nn.ReLU()\n",
        "    self.lrn2=nn.LocalResponseNorm(size=5,alpha=1e-4,beta=0.75,k=2.0)\n",
        "\n",
        "    #max pool 2\n",
        "    self.maxpool2=nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    #conv3 (gpu1->1/2, gpu2->1/2,  192 kernels for each gpu)\n",
        "    self.cnn3=nn.Conv2d(in_channels=256,out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu3=nn.ReLU()\n",
        "\n",
        "    #conv4 (gpu1->1, gpu2->2, 192 kernels for each gpu)\n",
        "    self.cnn4=nn.Conv2d(in_channels=384,out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu4=nn.ReLU()\n",
        "\n",
        "    #conv5 (gpu1->1, gpu2->2, 128 kernels for each gpu)\n",
        "    self.cnn5=nn.Conv2d(in_channels=384,out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu5=nn.ReLU()\n",
        "\n",
        "    #max pool 5\n",
        "    self.maxpool5=nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    #FC 1\n",
        "    self.fc1=nn.Linear(256*5*5,4096)\n",
        "    self.relu6=nn.ReLU()\n",
        "\n",
        "    #FC 2\n",
        "    self.fc2=nn.Linear(4096,4096)\n",
        "    self.relu7=nn.ReLU()\n",
        "\n",
        "    #FC 3\n",
        "    self.fc3=nn.Linear(4096,100)\n",
        "\n",
        "    #dropout\n",
        "    self.dropout=nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #conv1\n",
        "    out=self.cnn1(x)\n",
        "    out=self.relu1(out)\n",
        "    out=self.lrn1(out)\n",
        "\n",
        "    #max pooling 1\n",
        "    out=self.maxpool1(out)\n",
        "\n",
        "    #conv2\n",
        "    out=self.cnn2(out)\n",
        "    out=self.relu2(out)\n",
        "    out=self.lrn2(out)\n",
        "\n",
        "    #max pooling 2\n",
        "    out=self.maxpool2(out)\n",
        "\n",
        "    #conv3\n",
        "    out=self.cnn3(out)\n",
        "    out=self.relu3(out)\n",
        "\n",
        "    #conv4\n",
        "    out=self.cnn4(out)\n",
        "    out=self.relu4(out)\n",
        "\n",
        "    #conv3\n",
        "    out=self.cnn5(out)\n",
        "    out=self.relu5(out)\n",
        "\n",
        "    #max pooling 5\n",
        "    out=self.maxpool5(out)\n",
        "\n",
        "    # resize\n",
        "    #print(out.shape)\n",
        "    out=out.view(x.size(0),-1)\n",
        "\n",
        "    # fc 1\n",
        "    out=self.dropout(out)\n",
        "    out=self.fc1(out)\n",
        "    out=self.relu6(out)\n",
        "\n",
        "    # fc 2\n",
        "    out=self.dropout(out)\n",
        "    out=self.fc2(out)\n",
        "    out=self.relu7(out)\n",
        "\n",
        "    # fc 3\n",
        "    out=self.fc3(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Train & Test your own code\n",
        "\n",
        "model=Alexnet()\n",
        "model.to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate=0.01\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9,weight_decay=0.0005)\n",
        "\n",
        "def train(model, loader, criterion, optimizer,device):\n",
        "  model.train()\n",
        "  for inputs, targets in loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs,targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, loader, criterion, device, epoch):\n",
        "  model.eval()\n",
        "  total, correct, tot_loss =0,0,0\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in loader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs,targets)\n",
        "      tot_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += targets.size(0)\n",
        "      correct += (predicted == targets).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "\n",
        "  print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch,tot_loss, accuracy))\n",
        "\n",
        "\n",
        "num_epochs=10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train(model, train_loader, criterion, optimizer, device)\n",
        "  test(model, test_loader, criterion, device, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n193MtIicHfE",
        "outputId": "8a3c4d1b-71c0-4255-931d-be54d5795807"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 363.77926874160767. Accuracy: 1.4\n",
            "Epoch: 1. Loss: 357.3419985771179. Accuracy: 2.19\n",
            "Epoch: 2. Loss: 324.9289002418518. Accuracy: 5.75\n",
            "Epoch: 3. Loss: 306.14215660095215. Accuracy: 10.3\n",
            "Epoch: 4. Loss: 286.4503881931305. Accuracy: 13.73\n",
            "Epoch: 5. Loss: 267.70901322364807. Accuracy: 18.51\n",
            "Epoch: 6. Loss: 250.10759949684143. Accuracy: 23.26\n",
            "Epoch: 7. Loss: 230.69673943519592. Accuracy: 27.25\n",
            "Epoch: 8. Loss: 216.24664449691772. Accuracy: 30.6\n",
            "Epoch: 9. Loss: 199.9049003124237. Accuracy: 34.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpPLjwr8j9Zw"
      },
      "source": [
        "# Q2. Wiener Deconvolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWeEYLiSj9Zx"
      },
      "source": [
        "### Implementing Wiener deconvolution hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Dataset"
      ],
      "metadata": {
        "id": "4HHSOQ8blE6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1ATQh01anBo8ov--1ge8NWJM1Azl4HwfA/view?usp=sharing\n",
        "!unzip diffusercam_dataset.zip"
      ],
      "metadata": {
        "id": "-4NyJteas7Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import lpips\n",
        "from IPython.display import display, clear_output\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LossFunction(nn.Module):\n",
        "    \"\"\"Loss function class for multiple loss function.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.criterion_mse = nn.MSELoss()\n",
        "        self.criterion_lpips = lpips.LPIPS(net='vgg')\n",
        "\n",
        "    def forward(self, output, label, epoch=0):\n",
        "        lpips_loss = torch.mean(self.criterion_lpips.to(output.device)(output, label))\n",
        "        mix_loss = self.criterion_mse(output, label)\n",
        "        loss = mix_loss * 1 + lpips_loss * 0.05\n",
        "        return loss\n",
        "\n",
        "class WallerDataset(Dataset):\n",
        "    def __init__(self, path, train=False, transform_raw=None, transform_lab=None):\n",
        "        self.path = path\n",
        "        self.transform_raw = transform_raw\n",
        "        self.transform_lab = transform_lab\n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw_path = (self.path + '/raw' + '/' + f'im{idx+2}.npy')\n",
        "        lab_path = (self.path + '/label' + '/' + f'im{idx+2}.npy')\n",
        "        raw = np.load(raw_path)\n",
        "        raw = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        lab = np.load(lab_path)\n",
        "        lab = cv2.cvtColor(lab, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform_raw is not None:\n",
        "            raw = self.transform_raw(raw)\n",
        "        if self.transform_lab is not None:\n",
        "            lab = self.transform_lab(lab)\n",
        "\n",
        "        return raw, lab\n",
        "\n",
        "def PSNR(img1, img2):\n",
        "    mse = torch.mean((img1 - img2) ** 2, dim=(1,2,3), keepdim=True)\n",
        "    if torch.mean(mse) == 0:\n",
        "        return \"Same Image\"\n",
        "    return torch.mean(10 * torch.log10(1. / mse))"
      ],
      "metadata": {
        "id": "0C9n1zjyf87p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_window(size, fwhm):\n",
        "    with torch.no_grad():\n",
        "        sigma = size / fwhm\n",
        "        x = torch.arange(size) - (size - 1) / 2\n",
        "        gauss = torch.exp(-0.5 * (x / sigma) ** 2)\n",
        "    return gauss.detach()\n",
        "\n",
        "def gaus_t(x, fwhm=2):\n",
        "    b, c, w, h = x.size()\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    ga_w = gaussian_window(w, fwhm)\n",
        "    ga_h = gaussian_window(h, fwhm)\n",
        "\n",
        "    # 외적을 이용하여 2D 가우시안 윈도우 생성\n",
        "    ga = ga_w.unsqueeze(1) * ga_h.unsqueeze(0)\n",
        "    ga = ga.unsqueeze(0).unsqueeze(0)  # (1, 1, w, h)\n",
        "\n",
        "    return x * ga.to(x.device)\n",
        "\n",
        "\n",
        "class Wiener(nn.Module):\n",
        "    def __init__(self, psf, height, width, height_p, width_p, weights=0.01):\n",
        "        super(Wiener, self).__init__()\n",
        "        self.height_freq = height + height_p\n",
        "        self.width_freq = (width + width_p)// 2 + 1\n",
        "\n",
        "        # Set parameters as learnable\n",
        "        self.psf = psf\n",
        "        self.alpha = 0.1\n",
        "        self.delta = weights\n",
        "\n",
        "    def forward(self, raw: torch.Tensor, epsilon=1e-6) -> torch.Tensor:\n",
        "        psf = self.psf\n",
        "        B, C, H, W = raw.shape\n",
        "        _, _, H_p, W_p = psf.shape\n",
        "        psf = psf.reshape(1,-1,psf.size(-2),psf.size(-1))\n",
        "        psf_sum = psf.sum(dim=(-3, -2, -1), keepdim=True)          # (K, C, 1, 1)\n",
        "        psf_normalized = psf / (abs(psf_sum) * self.alpha + 1e-12)  # Prevent division by zero\n",
        "\n",
        "        # Apply symmetric padding to raw input\n",
        "        raw_padded = F.pad(\n",
        "            raw,\n",
        "            (W_p // 2, W_p - W_p // 2, H_p // 2, H_p - H_p // 2),\n",
        "            mode='replicate'\n",
        "        )\n",
        "\n",
        "        raw_padded = gaus_t(raw_padded, fwhm=2)\n",
        "\n",
        "        psf_padded = F.pad(\n",
        "            psf_normalized,\n",
        "            (W // 2, W - W // 2, H // 2, H - H // 2),\n",
        "            mode='constant'\n",
        "        )\n",
        "\n",
        "        raw_fft = torch.fft.rfft2(raw_padded, dim=(-2, -1))  # Shape: (B, C, H_freq, W_freq)\n",
        "        psf_fft = torch.fft.rfft2(psf_padded, s=(raw_padded.size(-2), raw_padded.size(-1)), dim=(-2, -1))  # Shape: (B, C, H_freq, W_freq)\n",
        "\n",
        "        wiener_filter = psf_fft.conj() / (psf_fft.abs()**2 + epsilon + self.delta)\n",
        "        out_fft = raw_fft * wiener_filter  # (B, C, H, W//2+1)\n",
        "\n",
        "        out_spatial = torch.fft.irfft2(out_fft, dim=(-2, -1))\n",
        "        out_spatial = torch.fft.ifftshift(out_spatial, dim=(-2, -1))\n",
        "        start_H = H_p // 2\n",
        "        start_W = W_p // 2\n",
        "        out_cropped = out_spatial[..., start_H:start_H + H, start_W:start_W + W]  # Shape: (K, B, C, H, W)\n",
        "\n",
        "        return out_cropped.real"
      ],
      "metadata": {
        "id": "iP44M_K9mtUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 1\n",
        "channels   = 1\n",
        "height, width     = 270, 480    # 원본 이미지 크기\n",
        "height_p, width_p = 270, 480    # PSF 커널 크기\n",
        "alpha      = 1                  # Wiener weight (alpha)\n",
        "\n",
        "psf = cv2.imread('./diffusercam_dataset/psf.tiff', 0)\n",
        "raw = np.load('./diffusercam_dataset/raw/im65.npy')\n",
        "img = np.load('./diffusercam_dataset/label/im65.npy')\n",
        "\n",
        "psf = cv2.resize(psf, (480,270))\n",
        "raw = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "raw = torch.tensor(raw).permute(2,0,1).unsqueeze(0).to(device) # raw: (B, C, H, W)\n",
        "psf = torch.tensor(psf).unsqueeze(0).unsqueeze(0).to(device)\n",
        "img = torch.tensor(img).permute(2,0,1).unsqueeze(0).to(device)\n",
        "psf = psf / 255.\n",
        "psf_bg = torch.mean(psf[:,:,0 : 15, 0 : 15])\n",
        "psf = psf-psf_bg\n",
        "psf[psf<0] = 0\n",
        "\n",
        "\n",
        "model = Wiener(\n",
        "    psf = psf,\n",
        "    height=height,\n",
        "    width=width,\n",
        "    height_p=height_p,\n",
        "    width_p=width_p,\n",
        "    weights=alpha,\n",
        ").to(device)\n",
        "\n",
        "out = model(raw)\n",
        "out = out / out.max()\n",
        "out[out<0] = 0\n",
        "print(\"Output shape:\", out.shape)\n",
        "\n",
        "plt.imshow(img[0].permute(1,2,0).cpu().detach())\n",
        "plt.show()\n",
        "plt.imshow(raw[0].permute(1,2,0).cpu().detach())\n",
        "plt.show()\n",
        "plt.imshow(out[0].permute(1,2,0).cpu().detach())\n",
        "plt.show()\n",
        "\n",
        "out_before_learn_ = out"
      ],
      "metadata": {
        "id": "RMWQRUCEf5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "transformer = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset_load = WallerDataset('./diffusercam_dataset', train=True, transform_raw=transformer, transform_lab=transformer)\n",
        "\n",
        "trainset_loader = torch.utils.data.DataLoader(\n",
        "    trainset_load,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "testset_loader = torch.utils.data.DataLoader(\n",
        "    trainset_load,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "\n",
        "\n",
        "# Train & Test your own code"
      ],
      "metadata": {
        "id": "bYA0NSI5f-tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXOuP9hvnJfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}